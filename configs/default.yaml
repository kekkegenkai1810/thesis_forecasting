random_seed: 42
# --- shim keys used by YOUR preprocess scripts ---
data_path: "data"
raw_hourly_csv: "data/raw/AT_hourly_MW_and_Price.csv"
capacity_csv: "data/raw/Installed Capacity per Production Type_201701010000-202301010000.csv"
holidays_csv: "data/AT_public_holidays_2017_2022.csv"
engineered_csv: "data/processed/AT_engineered.csv"

paths:
  engineered_csv: "data/processed/AT_engineered.csv"   # <- output of 01_preprocess_build_features.py
  interim_dir: "data/interim"
  processed_dir: "data/processed"
  checkpoints_dir: "checkpoints"
  outputs_dir: "outputs"

time:
  tz_output: "UTC"
  split:
    train_until: "2020-12-31 23:00:00"
    val_until:   "2021-12-31 23:00:00"
    test_until:  "2022-12-31 23:00:00"

# Map your engineered CSV headers to canonical names the pipeline will use
columns:
  timestamp: "Time (UTC)"
  wind_mw: "Wind_MW"
  solar_mw: "Solar_MW"
  load_mw: "Actual_Load_MW"
  price_eur_mwh: "Price_EUR_MWh"
  cf_wind: "CF_Wind"
  cf_solar: "CF_Solar"
  cap_wind_mw: "Wind_Cap_MW"
  cap_solar_mw: "Solar_Cap_MW"
  # optional holiday columns present in engineered CSV (adjust to yours)
  holiday_flags: ["is_public_holiday","is_weekend","is_special_day"]

features:
  context_hours: 168
  horizon_hours: 12
  scale_targets: false  # we train on native units/CF

training:
  encoder:
    latent_channels: 256
    lr: 0.001
    batch_size: 128
    epochs: 20
    price_loss_weight: 2.0
  elm:
    hidden: 1024
    ridge_lambda: 1e-2
    ensemble: 5

eval:
  spike_quantile: 0.9

asp:
  ramp_percentile: 0.99
  pv_night_hours: [0,1,2,3,4,21,22,23]
  
  

